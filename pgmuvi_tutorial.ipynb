{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0ivJ6YhNOdvWvy6LzMZdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ICSM/pgmuvi/blob/main/pgmuvi_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use pgmuvi - a brief introduction\n",
        "\n"
      ],
      "metadata": {
        "id": "zDG3hKiVmHUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pgmuvi` is a package to infer the multiwavelength behaviour of astronomical lightcurves using Gaussian Processes. It uses the `torch` and `gpytorch` libraries to define GP models, and the probabilistic programming language `pyro` to perform optimisation and sampling of the parameter space. \n",
        "\n",
        "## Why pgmuvi\n",
        "\n",
        "Many packages exist to interpret lightcurves with GPs, so why do we need another one? `pgmuvi` builds on the latest highly-optimised libraries which allow inference with arbitrary GP kernels. This makes it feasible to implement models that are very general, which is important in astronomy where lightcurves can have a wide range of different properties. However, this normally results in a speed penalty, since naive GPs scale with $\\mathcal{O}\\left(n^3\\right)$; to combat this, the libraries implement scalable approaches to GP inference, such as GPU computing. This allows both a wide range of GP kernels and fast inference, meaning that `pgmuvi` can be applied to arbitrary astronomical lightcurves without taking _that much_ longer than codes which are restricted to special cases but blisteringly fast in those cases.\n",
        "\n",
        "## What does pgmuvi do\n",
        "\n",
        "The objective is to understand what the PSD of the variability is, and in the case of (quasi-)periodic variables determine their dominant periods. To do this, `pgmuvi` uses a `SpectralMixtureKernel`, which models the PSD of the covariances of the GP as a Gaussian Mixture model. This is a very flexible model which is able to represent a wide range of different kinds of behaviour, so can be useful for a wide range of astronomical lightcurves.\n",
        "\n",
        "## How to use pgmuvi\n",
        "\n",
        "The rest of this notebook is basically a quickstart guide to `pgmuvi`. It generates sythetic observations, but you can replace these with reading in your own data and passing it to `pgmuvi`'s `Lightcurve` classes to apply it to your own data."
      ],
      "metadata": {
        "id": "hEyug9B9LHZC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WlvocyamEnT"
      },
      "outputs": [],
      "source": [
        "try: #This won't work right now - instead clone the repository and `pip install -e .`\n",
        "    import pgmuvi\n",
        "except ImportError:\n",
        "    !pip install pgmuvi\n",
        "    import pgmuvi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have imported/installed pgmuvi, we can use it to fit some data. For the purposes of this tutorial, we will generate two synthetic datasets - a single-wavelength lightcurve with very simple behaviour, and a second multi-wavelength one with some more complex patterns in it. It is important to note that the data must be in the form of `torch` tensors, rather than numpy arrays or other data types. "
      ],
      "metadata": {
        "id": "G9zGgK85MpGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\"\"\" Let's generate some synthetic data from a perturbed sine curve \n",
        "    but on the same time sampling as the real data\"\"\"\n",
        "\n",
        "P = np.random.uniform(30, 300)#137. #Days!\n",
        "print(\"True period: \",P,\" days\")\n",
        "n_data = 400\n",
        "jd_min = 2450000\n",
        "n_periods = np.random.uniform(3,10)\n",
        "jd_max = jd_min + P*(n_periods)\n",
        "print(\"Simulating for \",n_periods,\" periods\")\n",
        "\n",
        "#train_mag = \n",
        "#train_mag = train_mag + 0.1*torch.randn_like(train_mag)\n",
        "#train_mag_err = 0.1*train_mag\n",
        "\n",
        "period_guess = P*(np.random.uniform()+0.5)#147 #this number is in the same units as our original input.\n",
        "\n",
        "#generate data from a simple case - superimpose two sine curves and add noise\n",
        "timestamps_1d = torch.Tensor(np.random.uniform(jd_min, jd_max, size=n_data))#generate random x data here\n",
        "fluxes_1d = torch.sin(timestamps_1d*(2*np.pi/P))#generate random y data here\n",
        "fluxes_1d += 0.1*torch.randn_like(fluxes_1d)\n",
        "flux_err_1d = 0.1*fluxes_1d\n",
        "\n",
        "\n",
        "#now generate some data from a more complex case, maybe drawing from a specific PSD to test if pgmuvi reconstructs it\n",
        "#timestamps_2d = #generate random x data here\n",
        "#wavelengths_2d = #generate random x data here\n",
        "#fluxes_2d = #generate random y data here\n",
        "#flux_err_2d = "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7iXofH7Nbz_",
        "outputId": "448aad36-6952-490d-c2de-246cafaf5605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True period:  261.90817683645514  days\n",
            "Simulating for  3.505045582883577  periods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got some simple data, we can feed it to pgmuvi and see how it handles it."
      ],
      "metadata": {
        "id": "jP7ZjHrTOaWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmuvi.lightcurve import Lightcurve\n",
        "\n",
        "lightcurve_1d = Lightcurve(timestamps_1d, fluxes_1d, yerr = flux_err_1d, xtransform='minmax')"
      ],
      "metadata": {
        "id": "L1GzKnLJOzHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `xtransform` argument is important. The computing time and numerical stability of this GP model depends somewhat on the range of frequencies it is asked to compute the PSD on, so it is common to transform the inputs to some simple interval that ensures the frequencies are easier. Astronomical timeseries often cover very long times, and therefore very low frequencies if left in their original units (e.g. Julian Dates), which are particularly difficult to compute on. By default, `Lightcurve` therefore transforms the data so that it is in the closed interval $\\left[0, 1\\right]$, although other transformations (or none) can be defined by the user.\n",
        "\n",
        "Now that we have a lightcurve, we can try to fit it:"
      ],
      "metadata": {
        "id": "m06s-TKiPtfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightcurve_1d.fit(model='1D', likelihood='learn')"
      ],
      "metadata": {
        "id": "qV5QuLYdPwTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is a very simple case, and we have not specified any of the extra keywords that `fit` exposes. You will probably want to specify an initial guess of the GP hyperparameters, and the default number of components in the mixture model for the PSD is 4. Each mixture has a mean, scale and weight for the Gaussian, and there is a mean function (a constant mean) and an extra free parameter to learn additional noise in the input data.\n",
        "\n",
        "Now that the fit is completed, you can manually inspect the results"
      ],
      "metadata": {
        "id": "lwIguoa3QAD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightcurve_1d.results"
      ],
      "metadata": {
        "id": "OwkXyfaIQGLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "but this output is quite ugly. Instead, you can call some routines to make informative summaries of them that are easier to read"
      ],
      "metadata": {
        "id": "F_jpbrtgQIdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightcurve_1d.print_results()"
      ],
      "metadata": {
        "id": "UaaEuY1nQNv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and you can also generate some plots of the output; for example the lightcurve with the fit results on top of it"
      ],
      "metadata": {
        "id": "vmeUoeWnQn5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightcurve_1d.plot()"
      ],
      "metadata": {
        "id": "SSqM2r0RQvnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or the maximum a-posteriori PSD that pgmuvi has inferred"
      ],
      "metadata": {
        "id": "ZQHFjRrAQzsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightcurve_1d.plot_psd()"
      ],
      "metadata": {
        "id": "CzIGiIXKQ5Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want more information, you can use MCMC to sample the posterior of the model (this is still being developed!)"
      ],
      "metadata": {
        "id": "hfr1QuByQRtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightcurve_1d.mcmc()"
      ],
      "metadata": {
        "id": "lcrZocNTQ8p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This takes a while, unless you run it using a GPU - if that works. That's a subject for a different tutorial. Now we've got some results from MCMC, we should take a look at them. First, we can look at the trace"
      ],
      "metadata": {
        "id": "U32sM-jCRfAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#internally, Lightcurve should call arviz and produce these outputs\n",
        "lightcurve_1d.summary()\n",
        "lightcurve_1d.plot_trace() \n",
        "lightcurve_1d.pairplot()"
      ],
      "metadata": {
        "id": "5JcU0askS_H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you plot the lightcurve and results, you can get samples drawn from the MCMC chains instead of the mean and credible interval of the MAP solution"
      ],
      "metadata": {
        "id": "ixYnFmy3TSZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightcurve_1d.plot(mcmc_samples = True)"
      ],
      "metadata": {
        "id": "9y7pAhhRTdHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and you can do the same thing with the PSD"
      ],
      "metadata": {
        "id": "1LM835UNK_Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightcurve_1d.plot_psd(mcmc_samples = True)"
      ],
      "metadata": {
        "id": "0nbn-pNILCAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}