{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference of the Light Curve PSD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how you can use MCMC with `pgmuvi` to get a posterior distribution of the PSD of a light curve. We will use the same type of data as in the [basic tutorial](pgmuvi_tutorial.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Bayesian inference?\n",
    "\n",
    "\n",
    "## Why MCMC?\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This tutorial\n",
    "\n",
    "This tutorial will cover the following topics:\n",
    "\n",
    "* How to set up a model for Bayesian inference of the PSD of a light curve\n",
    "* How to run MCMC\n",
    "* How to visualize the results\n",
    "\n",
    "It unfortunately cannot tell you everything there is to know about fitting timeseries data with MCMC using `pgmuvi`, but it aims to give you a good starting point for your own projects.\n",
    "\n",
    "### Some imports\n",
    "\n",
    "Before we do anything, we need to make sure that `pgmuvi` imports correctly, and if it doesn't we need to install it. We also import some other packages that we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: #This won't work right now - instead clone the repository and `pip install -e .`\n",
    "    import pgmuvi\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/ICSM/pgmuvi.git\n",
    "    import pgmuvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.multiprocessing.get_all_sharing_strategies()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data\n",
    "\n",
    "Now that we have imported `pgmuvi`, we can create some data to fit. \n",
    "We will use the same type of data as in the [basic tutorial](pgmuvi_tutorial.ipynb), but we will use a different random seed to get different data. \n",
    "This data is drawn from a sine wave with a randomly-chosen period between 30 and 300 days, and a Gaussian noise component with a standard deviation of 0.1 times the absolute value of the flux. The times are randomly chosen to cover between 3 and 10 periods of the sine wave, with 40 points in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "\"\"\" Let's generate some synthetic data from a perturbed sine curve\n",
    "    but on the same time sampling as the real data\"\"\"\n",
    "\n",
    "P = np.random.uniform(30, 300)#137. #Days!\n",
    "print(\"True period: \",P,\" days\")\n",
    "n_data = 40\n",
    "jd_min = 2450000\n",
    "n_periods = np.random.uniform(3,10)\n",
    "jd_max = jd_min + P*(n_periods)\n",
    "print(\"Simulating for \",n_periods,\" periods\")\n",
    "\n",
    "#train_mag =\n",
    "#train_mag = train_mag + 0.1*torch.randn_like(train_mag)\n",
    "#train_mag_err = 0.1*train_mag\n",
    "\n",
    "period_guess = P*(np.random.uniform()+0.5)#147 #this number is in the same units as our original input.\n",
    "\n",
    "#generate data from a simple case - superimpose two sine curves and add noise\n",
    "timestamps_1d = torch.sort(torch.Tensor(np.random.uniform(jd_min, jd_max, size=n_data)))[0]#generate random x data here\n",
    "fluxes_1d = torch.sin(timestamps_1d*(2*np.pi/P))#generate random y data here\n",
    "fluxes_1d += 0.1*torch.randn_like(fluxes_1d)\n",
    "flux_err_1d = 0.1*fluxes_1d.abs()\n",
    "print(\"Generated data with \",n_data,\" points\")\n",
    "print(\"Period guess: \",period_guess,\" days\")\n",
    "print(\"Period guess: \",period_guess/P,\" periods\")\n",
    "print(timestamps_1d)\n",
    "print(fluxes_1d)\n",
    "print(flux_err_1d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Lightcurve object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmuvi.lightcurve import Lightcurve\n",
    "\n",
    "lightcurve_1d = Lightcurve(timestamps_1d, fluxes_1d, yerr = flux_err_1d, xtransform='minmax')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Model\n",
    "\n",
    "Now we can create our model. \n",
    "This is very similar to the previous tutorial, but with one small complication. \n",
    "When we didn't use MCMC, we wanted to learn additional diagonal noise to account for the intrinsic scatter of the data even if we had uncertainties on the data. \n",
    "However, when we use MCMC, we need to be careful about how we define our likelihood.\n",
    "If we attempt to learn this additional noise, `gpytorch` will inject `NaN`s along the diagonal of the covariance matrix, which will cause the MCMC sampler to fail. \n",
    "However, if you don't have uncertainty information, you can still learn this additional noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't work! Learning the additional noise results in NaNs in the covariance matrix during MCMC\n",
    "# lightcurve_1d.set_model(model='1D', likelihood='learn', num_mixtures=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_1d.set_model(model='1D', num_mixtures=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_1d.print_parameters()\n",
    "print(period_guess)\n",
    "guess = {\n",
    "         'sci_kernel.mixture_means': torch.Tensor([1/period_guess]),}\n",
    "lightcurve_1d.set_hypers(guess)\n",
    "lightcurve_1d.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lightcurve_1d._xdata_transformed)\n",
    "print(lightcurve_1d._ydata_transformed)\n",
    "print(lightcurve_1d._yerr_transformed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    "lightcurve_1d.mcmc(num_samples=100, #0, \n",
    "                   warmup_steps=100, \n",
    "                   num_chains=1)#num_samples=1000, burnin=100, thin=10, num_chains=4, num_steps=10, num_processes=4)\n",
    "#except Exception as e:\n",
    "    #print(e)\n",
    "    #lightcurve_1d.mcmc(num_samples=100, num_chains=2)#num_samples=1000, burnin=100, thin=10, num_chains=4, num_steps=10, num_processes=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to make a quick summary of the results.\n",
    "We can do this by plotting the marginal posterior distributions of each parameter, and by printing out the mean and standard deviation of each parameter. \n",
    "The mean and standard deviation are useful because they give us a sense of the \"best fit\" parameters, and the uncertainty on those parameters - or more formally, the credible interval of the parameters and the point estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_1d.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to take a look at how the MCMC sampler explored the parameter space. \n",
    "This is often referred to as the \"trace\" of the MCMC sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_1d.plot_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_1d.plot_corner()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the marginal posterior distributions is easy with `plot_pair`. `pgmuvi` often works best with `kind='scatter'`, but you can also try out `kind='kde'` or `kind='hexbin'` for prettier plots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to plot the light curve with samples from the posterior distribution of the model. \n",
    "This lets us see how well the model fits the data, and how well the model can predict the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot goes here\n",
    "lightcurve_1d.plot(mcmc_samples=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to look at the posterior distribution of the PSD. \n",
    "This is useful to understand how much the data constrain the PSD, and hence how much we can trust the results. \n",
    "The same information is effectively contained in the posterior distribution of the model parameters (as reported in the summary or displayed in the trace and the corner plot), but it is often easier to understand the PSD directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot goes here\n",
    "lightcurve_1d.plot_psd(mcmc_samples=True, log=(True, True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgmuvi_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
